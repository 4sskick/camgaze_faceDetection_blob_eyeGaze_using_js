

\documentclass[annual]{acmsiggraph}
\usepackage{relsize}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}

\TOGonlineid{}
\TOGvolume{}
\TOGnumber{}
\TOGarticleDOI{}
\TOGprojectURL{}
\TOGvideoURL{}
\TOGdataURL{}
\TOGcodeURL{}

\newcommand{\Acronym}[1]{\ensuremath{{\small{\texttt{#1}}}}}
\newcommand{\Name}{\Acronym{Camgaze.js}}
\newcommand{\False}{\Constant{false}}
\newcommand{\True}{\Constant{true}}
\newcommand{\Symbol}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\Function}[1]{\ensuremath{{\small \textsc{#1}}}}
\newcommand{\Constant}[1]{\ensuremath{\small{\texttt{#1}}}}
\newcommand{\Var}[1]{\ensuremath{{\small{\textsl{#1}}}}}
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}

\title{Camgaze.js : Mobile Eye Tracking and Gaze Prediction in JavaScript}
\author{Alex Wallar \\ Christian Poellabauer \\ Patrick Flynn}
\pdfauthor{Alex Wallar}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

\section{Challenges}

\section{Implementation}

$\Name$ goes through two steps in order to predict the gaze direction. 
Firstly, $\Name$ detects each pupil. It then uses the pupils deviation 
from a unique point on the face to determine the gaze metric, \Symbol{G}. 
This metric needs to be calibrated in order for there to be a mapping 
from \Symbol{G} to a point on the screen. Once this gaze metric has been 
calibrated, $\Name$ should be able to interpolate area of the 
screen the user is looking at. A high level description of the 
algortihm is shown below.

\begin{algorithm}
\caption{Pseudocode for $\Name$}
\label{algo:Main}
\begin{algorithmic}[1]
\setcounter{ALC@line}{0}

\vspace*{1mm}

\STATE $\Symbol{F} \leftarrow \Function{InitGazeMapping}()$

\WHILE{$\Function{StillCalibrating()} == \True$}
\STATE $P_{list} \leftarrow \Function{DetectPupils()}$
\STATE $\Symbol{G} \leftarrow \Function{DetermineGazeMetric}(P_{list})$
\STATE $\Symbol{F} \leftarrow \Function{Calibrate}(\Symbol{G}, \Symbol{F})$
\ENDWHILE

\WHILE{$\Function{SessionFinished()} == \False$}
\STATE $P_{list} \leftarrow \Function{DetectPupils()}$
\STATE $\Symbol{G} \leftarrow \Function{DetermineGazeMetric}(P_{list})$
\STATE $\Function{ProjectGazeOntoScreen}(\Symbol{F}(\Symbol{G}))$
\ENDWHILE

\end{algorithmic}
\end{algorithm}

\subsection{Pupil Detection}

Detecting the pupils enables $\Name$ to determine the gaze direction.
Pupil detection in this approach is aimed to be fast in order to 
be deployable onto mobile devices. Firstly, the frame is converted to 
grayscale and the eye is detected using the Viola-Jones Object
Detection Framework \cite{Viola01}. The region of interest (ROI) is
then thresholded for an array of different colors and blob detection
takes place. All of the detected connected components are stored as 
possible pupils. Out of these possible pupils, the one with the 
minimum overall error is designated as the pupil. Below are the 
expressions to be minimized.

\begin{eqnarray}
\Function{err}_{\alpha}(p) &=& \frac{
    \mathlarger{\sum}_{c \in Corners}{
        \vert 
            \frac{\pi}{4} - \Function{arctan}(
                \vert \frac{p_y - c_y}{p_x - c_x} \vert
            ) 
        \vert
    }
}{\pi} \\
\Function{err}_{size}(p) &=& \frac{
     \vert
        \Var{avgPupilSize} - \Function{size}(p)
    \vert
}{2}
\end{eqnarray}

$\Function{err}_{\alpha}$ refers to how far the blob center is from the
center of the Haar bounding rectangle. We use angle deviation instead
of pixel distance for this metric because we assume that the pupil would
not always reside to close to the center and a direct pixel distance
might yield other blobs more suitable. The angle deviation acts a weak
error function in order to be more lenient without the use of constants.
Once the blob with the minimum error is extracted, the center of the blob
is returned.

\subsection{Determining the Gaze Metric}

The gaze metric is determined by establishing a reference point that will
in a constant position with reference to the pupil center. Using this point,
we are able to capture the motion of the pupil without the influence of
head movement or tablet jitter.

\section{Methodology}

\section{Applications}

\section{Discussion}

\section{Limitations}

\section{Future Research}

\bibliographystyle{acmsiggraph}
\bibliography{paper}

\end{document}
