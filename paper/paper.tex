

\documentclass[annual]{acmsiggraph}
\usepackage{relsize}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}

\TOGonlineid{}
\TOGvolume{}
\TOGnumber{}
\TOGarticleDOI{}
\TOGprojectURL{}
\TOGvideoURL{}
\TOGdataURL{}
\TOGcodeURL{}

\newcommand{\Acronym}[1]{\ensuremath{{\small{\texttt{#1}}}}}
\newcommand{\Name}{\Acronym{Camgaze.js}}
\newcommand{\False}{\Constant{false}}
\newcommand{\True}{\Constant{true}}
\newcommand{\Symbol}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\Function}[1]{\ensuremath{{\small \textsc{#1}}}}
\newcommand{\Constant}[1]{\ensuremath{\small{\texttt{#1}}}}
\newcommand{\Var}[1]{\ensuremath{{\small{\textsl{#1}}}}}
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}

\title{Camgaze.js : Mobile Eye Tracking and Gaze Prediction in JavaScript}
\author{Alex Wallar \\ Christian Poellabauer \\ Patrick Flynn}
\pdfauthor{Alex Wallar}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Eye tracking is a difficult problem that is usually solved using specialised
hardware and therefore has limited availability due to cost and deployment
difficulties. We describe Camgaze.js – a client-side Javascript library that
is able to measure the point of gaze using only commodity optical cameras.
In our work, we conduct experiments using Camgaze.js to show the usability
of such a system. We also discuss the challenges and applications of using
an in browser eye tracking system. Since the described eye tracker works
inside the browser without any additional installation setup, it provides
a more feasible scope of use.
\end{abstract}

\section{Introduction}

Eye tracking is a challenging problem, that has been attempted to be solved
since the 18th century (Ahrens, A., 1891. Die Bewegung der Augen beim Schreiben.
Rostock: University of Rostock). Currently, it is mostly viewed as a proble
in Computer Vision.
The majority of eye tracking solutions available on the market today are a combination
of software and specialised hardware. The principles behind hardware varies greatly:
it ranges from head-mounted cameras to lenses with integrated coils. It is believed
that the state of the art solutions can allow accuracy up to NN\% {link}. Despite that,
carrying out eye tracking experiments remains an issue – it is expensive, requires
complicated deployment and calibration and, in most cases, has to be carried out
in a controlled environment.

In the recent years, it has been shown (Agustin, 2009; Sewell and Komogortsev, 2010)
that it is possible to use commodity cameras, often built into modern computers to
perform eye tracking with promising quality. Deployment of such systems is relatively
simple, but in the described cases it is tied to specific computer platforms \cite{holland2012eye}.

Web applications are “rich” websites that are able to run without external plugins inside the browser. 

\section{Challenges}

\section{Implementation}

$\Name$ goes through two steps in order to predict the gaze direction. 
Firstly, $\Name$ detects each pupil. It then uses the pupils deviation 
from a unique point on the face to determine the gaze metric, \Symbol{G}. 
This metric needs to be calibrated in order for there to be a mapping 
from \Symbol{G} to a point on the screen. Once this gaze metric has been 
calibrated, $\Name$ should be able to interpolate area of the 
screen the user is looking at. A high level description of the 
algortihm is shown below.

\begin{algorithm}
\caption{Pseudocode for $\Name$}
\label{algo:Main}
\begin{algorithmic}[1]
\setcounter{ALC@line}{0}

\vspace*{1mm}

\STATE $\Symbol{F} \leftarrow \Function{InitGazeMapping}()$

\WHILE{$\Function{StillCalibrating()} == \True$}
\STATE $P_{list} \leftarrow \Function{DetectPupils()}$
\STATE $\Symbol{G} \leftarrow \Function{DetermineGazeMetric}(P_{list})$
\STATE $\Symbol{F} \leftarrow \Function{Calibrate}(\Symbol{G}, \Symbol{F})$
\ENDWHILE

\WHILE{$\Function{SessionFinished()} == \False$}
\STATE $P_{list} \leftarrow \Function{DetectPupils()}$
\STATE $\Symbol{G} \leftarrow \Function{DetermineGazeMetric}(P_{list})$
\STATE $\Function{ProjectGazeOntoScreen}(\Symbol{F}(\Symbol{G}))$
\ENDWHILE

\end{algorithmic}
\end{algorithm}

\subsection{Pupil Detection}

Detecting the pupils enables $\Name$ to determine the gaze direction.
Pupil detection in this approach is aimed to be fast in order to 
be deployable onto mobile devices. Firstly, the frame is converted to 
grayscale and the eye is detected using the Viola-Jones Object
Detection Framework \cite{Viola01}. The region of interest (ROI) is
then thresholded for an array of different colors and blob detection
takes place. All of the detected connected components are stored as 
possible pupils. Out of these possible pupils, the one with the 
minimum overall error is designated as the pupil. Below are the 
expressions to be minimized.

\begin{eqnarray}
\Function{err}_{\alpha}(p) &=& \frac{
    \mathlarger{\sum}_{c \in Corners}{
        \vert 
            \frac{\pi}{4} - \Function{arctan}(
                \vert \frac{p_y - c_y}{p_x - c_x} \vert
            ) 
        \vert
    }
}{\pi} \\
\Function{err}_{size}(p) &=& \frac{
     \vert
        \Var{avgPupilSize} - \Function{size}(p)
    \vert
}{2}
\end{eqnarray}

$\Function{err}_{\alpha}$ refers to how far the blob center is from the
center of the Haar bounding rectangle. We use angle deviation instead
of pixel distance for this metric because we assume that the pupil would
not always reside to close to the center and a direct pixel distance
might yield other blobs more suitable. The angle deviation acts a weak
error function in order to be more lenient without the use of constants.
Once the blob with the minimum error is extracted, the center of the blob
is returned.

\subsection{Determining the Gaze Metric}

The gaze metric is determined by establishing a reference point that will
in a constant position with reference to the pupil center. Using this point,
we are able to capture the motion of the pupil without the influence of
head movement or tablet jitter.

\section{Methodology}

\section{Applications}

\section{Discussion}

\section{Limitations}

\section{Future Research}

\bibliographystyle{acmsiggraph}
\bibliography{paper}

\end{document}
